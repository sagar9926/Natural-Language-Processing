{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "END Assignment7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagar9926/Natural-Language-Processing/blob/main/END_Assignment7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYiRsFGD6iUC"
      },
      "source": [
        "# 0 TorchText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgO2M09rekNe",
        "outputId": "87fc23fd-1bcb-4b68-ae95-4781df8deb6d"
      },
      "source": [
        "!git clone https://github.com/prrao87/fine-grained-sentiment.git"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'fine-grained-sentiment' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dn7zb2VTfIhi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bd5fd87-0b32-4e3d-8c82-ce1dcd0960c0"
      },
      "source": [
        "!mkdir data"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPB4rgYUeofd"
      },
      "source": [
        "!cp /content/fine-grained-sentiment/data/sst/sst_dev.txt /content/data\r\n",
        "!cp /content/fine-grained-sentiment/data/sst/sst_test.txt /content/data\r\n",
        "!cp /content/fine-grained-sentiment/data/sst/sst_train.txt /content/data\r\n",
        "   "
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWXKYd1jeuTt"
      },
      "source": [
        "# Import Library\r\n",
        "import random\r\n",
        "import torch, torchtext\r\n",
        "from torchtext import data\r\n",
        "from torchtext import datasets\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# Manual Seed\r\n",
        "SEED = 43\r\n",
        "torch.manual_seed(SEED)\r\n",
        "import spacy\r\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlSbEgSYgJrq"
      },
      "source": [
        "df_train = pd.read_csv('/content/data/sst_train.txt', sep='\\t', header=None,\r\n",
        "                   names=['sentiment', 'review'],\r\n",
        "                  )\r\n",
        "df_train['sentiment'] = df_train['sentiment'].str.replace('__label__', '')\r\n",
        "df_train['sentiment'] = df_train['sentiment'].astype(int)\r\n",
        "\r\n",
        "df_test = pd.read_csv('/content/data/sst_test.txt', sep='\\t', header=None,\r\n",
        "                   names=['sentiment', 'review'],\r\n",
        "                  )\r\n",
        "df_test['sentiment'] = df_test['sentiment'].str.replace('__label__', '')\r\n",
        "df_test['sentiment'] = df_test['sentiment'].astype(int)\r\n",
        "\r\n",
        "\r\n",
        "df_dev = pd.read_csv('/content/data/sst_dev.txt', sep='\\t', header=None,\r\n",
        "                   names=['sentiment', 'review'],\r\n",
        "                  )\r\n",
        "df_dev['sentiment'] = df_dev['sentiment'].str.replace('__label__', '')\r\n",
        "df_dev['sentiment'] = df_dev['sentiment'].astype(int)\r\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "vTf1AYrKgYsV",
        "outputId": "eeae4f76-cdc2-415c-f508-7342f06335ef"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>Singer/composer Bryan Adams contributes a slew...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>You 'd think by now America would have had eno...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Yet the act is still charming here .</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment                                             review\n",
              "0          4  The Rock is destined to be the 21st Century 's...\n",
              "1          5  The gorgeously elaborate continuation of `` Th...\n",
              "2          4  Singer/composer Bryan Adams contributes a slew...\n",
              "3          3  You 'd think by now America would have had eno...\n",
              "4          4               Yet the act is still charming here ."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkF0KxSj-FM3"
      },
      "source": [
        "import re\r\n",
        "def cleanup_text(texts):\r\n",
        "    cleaned_text = []\r\n",
        "    for text in texts:\r\n",
        "        # remove punctuation\r\n",
        "        text = re.sub('[^a-zA-Z0-9]', ' ', text)\r\n",
        "        # remove multiple spaces\r\n",
        "        text = re.sub(r' +', ' ', text)\r\n",
        "        # remove newline\r\n",
        "        text = re.sub(r'\\n', ' ', text)\r\n",
        "        cleaned_text.append(text)\r\n",
        "    return cleaned_text"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCG43NeVho5l"
      },
      "source": [
        "Review = data.Field(sequential = True, tokenize = 'spacy' , batch_first =True, include_lengths=True,preprocessing = cleanup_text)\r\n",
        "Sentiment = data.LabelField(tokenize ='spacy', is_target=True, batch_first =True, sequential =False)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtDQzGX75w67"
      },
      "source": [
        "train_fields = [('review', Review),('sentiment',Sentiment)]\r\n",
        "dev_fields = [('review', Review),('sentiment',Sentiment)]\r\n",
        "test_fields = [('review', Review),('sentiment',Sentiment)]"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRnzmBpU6Oz9"
      },
      "source": [
        "train_example = [data.Example.fromlist([df_train.review[i],df_train.sentiment[i]], train_fields) for i in range(df_train.shape[0])] \r\n",
        "dev_example = [data.Example.fromlist([df_dev.review[i],df_dev.sentiment[i]], dev_fields) for i in range(df_dev.shape[0])] \r\n",
        "test_example = [data.Example.fromlist([df_test.review[i],df_test.sentiment[i]], test_fields) for i in range(df_test.shape[0])] "
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNj15-YP8b6S"
      },
      "source": [
        "trainDataset = data.Dataset(train_example, train_fields)\r\n",
        "devDataset = data.Dataset(dev_example, dev_fields)\r\n",
        "testDataset = data.Dataset(test_example, test_fields)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PSW-1dINmRJ",
        "outputId": "627b2079-bdca-4dc8-8c8f-bbe320dd8520"
      },
      "source": [
        "vars(trainDataset.examples[0])"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'review': ['The',\n",
              "  'Rock',\n",
              "  'is',\n",
              "  'destined',\n",
              "  'to',\n",
              "  'be',\n",
              "  'the',\n",
              "  '21st',\n",
              "  'Century',\n",
              "  ' s',\n",
              "  'new',\n",
              "  ' ',\n",
              "  ' ',\n",
              "  'Conan',\n",
              "  ' ',\n",
              "  'and',\n",
              "  'that',\n",
              "  'he',\n",
              "  ' s',\n",
              "  'going',\n",
              "  'to',\n",
              "  'make',\n",
              "  'a',\n",
              "  'splash',\n",
              "  'even',\n",
              "  'greater',\n",
              "  'than',\n",
              "  'Arnold',\n",
              "  'Schwarzenegger',\n",
              "  ' ',\n",
              "  'Jean',\n",
              "  ' ',\n",
              "  'Claud',\n",
              "  'Van',\n",
              "  'Damme',\n",
              "  'or',\n",
              "  'Steven',\n",
              "  'Segal',\n",
              "  ' '],\n",
              " 'sentiment': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAnX6AtwC7dH"
      },
      "source": [
        "### Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7Dz7JJfMqyC"
      },
      "source": [
        "def random_deletion(words, p=0.5): \n",
        "    if len(words) == 1: # return if single word\n",
        "        return words\n",
        "    remaining = list(filter(lambda x: random.uniform(0,1) > p,words)) \n",
        "    if len(remaining) == 0: # if not left, sample a random word\n",
        "        return [random.choice(words)] \n",
        "    else:\n",
        "        return remaining"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYnAgDSwEq3q",
        "outputId": "f7786ad9-fe49-4f3c-8958-f27fdedfb60a"
      },
      "source": [
        "random_deletion([1,2,3,4,5,6,7])"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 3, 4, 7]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnkbG15HO3Yj"
      },
      "source": [
        "def random_swap(words, n=5): \n",
        "    length = range(len(words)) \n",
        "    for _ in range(n):\n",
        "        idx1, idx2 = random.sample(length, 2)\n",
        "        words[idx1], words[idx2] = words[idx2], words[idx1] \n",
        "    return words"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRNEQIN6Ezkg",
        "outputId": "1e9b05d5-369e-4e08-ece9-ca1ff3929d05"
      },
      "source": [
        "random_swap([\"My\",\"Name\",\"Is\",\"Sagar\"])"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Is', 'Sagar', 'Name', 'My']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnaW77-7nxEh",
        "outputId": "4188a9f6-e4f4-4cc0-9008-858257af2f8a"
      },
      "source": [
        "!pip install google_trans_new"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: google_trans_new in /usr/local/lib/python3.6/dist-packages (1.1.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHhNBbYrRXNy"
      },
      "source": [
        "import random\n",
        "import google_trans_new\n",
        "from google_trans_new import google_translator\n",
        "\n",
        "def back_trans(words):\n",
        "  translator = google_translator()\n",
        "  sentence = \" \".join(words)\n",
        "\n",
        "  available_langs = list(google_trans_new.LANGUAGES.keys()) \n",
        "  trans_lang = random.choice(available_langs) \n",
        "  #print(f\"Translating to {googletrans.LANGUAGES[trans_lang]}\")\n",
        "\n",
        "  translations = translator.translate(sentence, lang_tgt=trans_lang) \n",
        "  \n",
        "  translations_en_random = translator.translate(translations, lang_src=trans_lang, lang_tgt='en')\n",
        "  print(translations_en_random) \n",
        "  \n",
        "  return translations_en_random.split(\" \")"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3cAfTnhFzk2",
        "outputId": "036e7cb5-387f-4c16-c92f-1d4c47cd718b"
      },
      "source": [
        "back_trans([\"The\" , \"dog\" , \"is\" , \"sleeping\", \"on\",\"mat\"])"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The dog is sleeping on the mat \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'dog', 'is', 'sleeping', 'on', 'the', 'mat', '']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zejlRyuw7-MM"
      },
      "source": [
        "def augmentation() :\r\n",
        "\r\n",
        "  ## we will apply augmentation only 50% of times\r\n",
        "\r\n",
        "  aug_funcs = [random_swap , back_trans,random_deletion]\r\n",
        "  p = np.random.uniform()\r\n",
        "  \r\n",
        "  if 0 < p < 0.5 :\r\n",
        "    func_index =  np.random.randint(0,3)\r\n",
        "    augment = aug_funcs[func_index]\r\n",
        "  else : \r\n",
        "    augment = False\r\n",
        "  return augment"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJT-sf-MIkbY"
      },
      "source": [
        "Review.build_vocab(trainDataset)\r\n",
        "Sentiment.build_vocab(trainDataset)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGPKHYFkI2FC",
        "outputId": "9abe8520-706e-4a5e-b0d3-08f49252cff3"
      },
      "source": [
        "print('Size of input vocab : ', len(Review.vocab))\r\n",
        "print('Size of label vocab : ', len(Sentiment.vocab))\r\n",
        "print('Top 10 words appreared repeatedly :', list(Review.vocab.freqs.most_common(10)))\r\n",
        "print('Labels : ', Sentiment.vocab.stoi)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of input vocab :  17144\n",
            "Size of label vocab :  5\n",
            "Top 10 words appreared repeatedly : [(' ', 22004), ('the', 6087), ('and', 4474), ('of', 4446), ('a', 4423), ('to', 3024), (' s', 2544), ('is', 2540), ('that', 1916), ('in', 1817)]\n",
            "Labels :  defaultdict(<function _default_unk_index at 0x7fd1b07f5158>, {4: 0, 2: 1, 3: 2, 5: 3, 1: 4})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k85dqY2WJAOP"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrBNcichJIAm"
      },
      "source": [
        "train_iterator, valid_iterator = data.BucketIterator.splits((trainDataset, devDataset), batch_size = 32, \r\n",
        "                                                            sort_key = lambda x: len(x.review),\r\n",
        "                                                            sort_within_batch=True, device = device)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAvtk-ntJZSR"
      },
      "source": [
        "import os, pickle\r\n",
        "with open('tokenizer.pkl', 'wb') as tokens: \r\n",
        "    pickle.dump(Review.vocab.stoi, tokens)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNnaKgoPJfy5"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "class classifier(nn.Module):\r\n",
        "    \r\n",
        "    # Define all the layers used in model\r\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\r\n",
        "        \r\n",
        "        super().__init__()          \r\n",
        "        \r\n",
        "        # Embedding layer\r\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\r\n",
        "        \r\n",
        "        # LSTM layer\r\n",
        "        self.encoder = nn.LSTM(embedding_dim, \r\n",
        "                           hidden_dim, \r\n",
        "                           num_layers=n_layers, \r\n",
        "                           dropout=dropout,\r\n",
        "                           batch_first=True)\r\n",
        "        # try using nn.GRU or nn.RNN here and compare their performances\r\n",
        "        # try bidirectional and compare their performances\r\n",
        "        \r\n",
        "        # Dense layer\r\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\r\n",
        "        \r\n",
        "    def forward(self, text, text_lengths):\r\n",
        "        \r\n",
        "        # text = [batch size, sent_length]\r\n",
        "        embedded = self.embedding(text)\r\n",
        "        # embedded = [batch size, sent_len, emb dim]\r\n",
        "      \r\n",
        "        # packed sequence\r\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\r\n",
        "        \r\n",
        "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\r\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\r\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\r\n",
        "    \r\n",
        "        # Hidden = [batch size, hid dim * num directions]\r\n",
        "        dense_outputs = self.fc(hidden)   \r\n",
        "        \r\n",
        "        # Final activation function softmax\r\n",
        "        output = F.softmax(dense_outputs[0], dim=1)\r\n",
        "            \r\n",
        "        return output"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1_c98xnKQfB"
      },
      "source": [
        "# Define hyperparameters\r\n",
        "size_of_vocab = len(Review.vocab)\r\n",
        "embedding_dim = 300\r\n",
        "num_hidden_nodes = 100\r\n",
        "num_output_nodes = 5\r\n",
        "num_layers = 2\r\n",
        "dropout = 0.2\r\n",
        "\r\n",
        "# Instantiate the model\r\n",
        "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes, num_output_nodes, num_layers, dropout = dropout)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iRD5ZAxKTpz",
        "outputId": "9dd9fa0d-f5f7-4569-8ff8-e4069756d2f1"
      },
      "source": [
        "print(model)\r\n",
        "\r\n",
        "#No. of trianable parameters\r\n",
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "    \r\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifier(\n",
            "  (embedding): Embedding(17144, 300)\n",
            "  (encoder): LSTM(300, 100, num_layers=2, batch_first=True, dropout=0.2)\n",
            "  (fc): Linear(in_features=100, out_features=5, bias=True)\n",
            ")\n",
            "The model has 5,385,305 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z60fnIU5KWcX"
      },
      "source": [
        "import torch.optim as optim\r\n",
        "\r\n",
        "# define optimizer and loss\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-4)\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "# define metric\r\n",
        "def binary_accuracy(preds, y):\r\n",
        "    #round predictions to the closest integer\r\n",
        "    _, predictions = torch.max(preds, 1)\r\n",
        "    \r\n",
        "    correct = (predictions == y).float() \r\n",
        "    acc = correct.sum() / len(correct)\r\n",
        "    return acc\r\n",
        "    \r\n",
        "# push to cuda if available\r\n",
        "model = model.to(device)\r\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H8s7Ip8LXZm"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\r\n",
        "    \r\n",
        "    # initialize every epoch \r\n",
        "    epoch_loss = 0\r\n",
        "    epoch_acc = 0\r\n",
        "    \r\n",
        "    # set the model in training phase\r\n",
        "    model.train()  \r\n",
        "\r\n",
        "    ## applying augmentation\r\n",
        "    for i in range(len(train_iterator.dataset.examples)):\r\n",
        "      if i%1000 == 0:\r\n",
        "        print(\"augment :\",i)\r\n",
        "      aug = augmentation()\r\n",
        "      if len(train_iterator.dataset.examples[i].review) > 10 :\r\n",
        "        if bool(aug): \r\n",
        "          print(\"before\",train_iterator.dataset.examples[i].review,len(train_iterator.dataset.examples[i].review))\r\n",
        "          train_iterator.dataset.examples[i].review = aug(train_iterator.dataset.examples[i].review) \r\n",
        "          print(\"after\",train_iterator.dataset.examples[i].review)\r\n",
        "\r\n",
        "    for batch in iterator:\r\n",
        "        \r\n",
        "        # resets the gradients after every batch\r\n",
        "        optimizer.zero_grad()   \r\n",
        "        \r\n",
        "        # retrieve text and no. of words\r\n",
        "        tweet, tweet_lengths = batch.review   \r\n",
        "        \r\n",
        "        # convert to 1D tensor\r\n",
        "        predictions = model(tweet, tweet_lengths).squeeze()\r\n",
        "        \r\n",
        "        # compute the loss\r\n",
        "        loss = criterion(predictions, batch.sentiment)        \r\n",
        "        \r\n",
        "        # compute the binary accuracy\r\n",
        "        acc = binary_accuracy(predictions, batch.sentiment)   \r\n",
        "        \r\n",
        "        # backpropage the loss and compute the gradients\r\n",
        "        loss.backward()       \r\n",
        "        \r\n",
        "        # update the weights\r\n",
        "        optimizer.step()      \r\n",
        "        \r\n",
        "        # loss and accuracy\r\n",
        "        epoch_loss += loss.item()  \r\n",
        "        epoch_acc += acc.item()    \r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Am-cmugIMNdw"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\r\n",
        "    \r\n",
        "    # initialize every epoch\r\n",
        "    epoch_loss = 0\r\n",
        "    epoch_acc = 0\r\n",
        "\r\n",
        "    # deactivating dropout layers\r\n",
        "    model.eval()\r\n",
        "    \r\n",
        "    # deactivates autograd\r\n",
        "    with torch.no_grad():\r\n",
        "    \r\n",
        "        for batch in iterator:\r\n",
        "        \r\n",
        "            # retrieve text and no. of words\r\n",
        "            tweet, tweet_lengths = batch.review\r\n",
        "            \r\n",
        "            # convert to 1d tensor\r\n",
        "            predictions = model(tweet, tweet_lengths).squeeze()\r\n",
        "            \r\n",
        "            # compute loss and accuracy\r\n",
        "            loss = criterion(predictions, batch.sentiment)\r\n",
        "            acc = binary_accuracy(predictions, batch.sentiment)\r\n",
        "            \r\n",
        "            # keep track of loss and accuracy\r\n",
        "            epoch_loss += loss.item()\r\n",
        "            epoch_acc += acc.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FxbDAxXMQNv",
        "outputId": "0fabae87-23a7-4295-c30d-97f780ab552a"
      },
      "source": [
        "N_EPOCHS = 10\r\n",
        "best_valid_loss = float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "     \r\n",
        "    # train the model\r\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\r\n",
        "    \r\n",
        "    # evaluate the model\r\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\r\n",
        "    \r\n",
        "    # save the best model\r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\r\n",
        "    \r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "augment : 0\n",
            "before ['The', 'Rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'Century', ' s', 'new', ' ', ' ', 'Conan', ' ', 'and', 'that', 'he', ' s', 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'Arnold', 'Schwarzenegger', ' ', 'Jean', ' ', 'Claud', 'Van', 'Damme', 'or', 'Steven', 'Segal', ' '] 39\n",
            "The Rock is destined to be the new 21st Century Conan and that he is going to make a bigger splash than Arnold Schwarzenegger Jean Claud Van Damme or Steven Segal \n",
            "after ['The', 'Rock', 'is', 'destined', 'to', 'be', 'the', 'new', '21st', 'Century', 'Conan', 'and', 'that', 'he', 'is', 'going', 'to', 'make', 'a', 'bigger', 'splash', 'than', 'Arnold', 'Schwarzenegger', 'Jean', 'Claud', 'Van', 'Damme', 'or', 'Steven', 'Segal', '']\n",
            "before ['Whether', 'or', 'not', 'you', ' re', 'enlightened', 'by', 'any', 'of', 'Derrida', ' s', 'lectures', 'on', ' ', ' ', 'the', 'other', ' ', 'and', ' ', ' ', 'the', 'self', ' ', ' ', 'Derrida', 'is', 'an', 'undeniably', 'fascinating', 'and', 'playful', 'fellow', ' '] 34\n",
            "after ['Whether', 'and', 'not', 'you', ' re', 'enlightened', 'Derrida', 'any', 'of', 'Derrida', ' s', 'the', 'on', ' ', 'fellow', 'lectures', 'playful', ' ', 'and', ' ', ' ', 'the', 'self', ' ', ' ', 'by', 'is', 'an', 'undeniably', 'fascinating', 'or', 'other', ' ', ' ']\n",
            "before ['Just', 'the', 'labour', 'involved', 'in', 'creating', 'the', 'layered', 'richness', 'of', 'the', 'imagery', 'in', 'this', 'chiaroscuro', 'of', 'madness', 'and', 'light', 'is', 'astonishing', ' '] 22\n",
            "Only the work of creating the layered richness of images in this sanctuary of madness and light is astonishing. \n",
            "after ['Only', 'the', 'work', 'of', 'creating', 'the', 'layered', 'richness', 'of', 'images', 'in', 'this', 'sanctuary', 'of', 'madness', 'and', 'light', 'is', 'astonishing.', '']\n",
            "before ['A', 'thunderous', 'ride', 'at', 'first', ' ', 'quiet', 'cadences', 'of', 'pure', 'finesse', 'are', 'few', 'and', 'far', 'between', ' ', 'their', 'shortage', 'dilutes', 'the', 'potency', 'of', 'otherwise', 'respectable', 'action', ' '] 27\n",
            "after ['A', 'at', 'first', 'of', 'pure', 'finesse', 'are', 'and', 'between', ' ', 'shortage', 'the', 'of', 'otherwise', 'action']\n",
            "before ['You', 'walk', 'out', 'of', 'The', 'Good', 'Girl', 'with', 'mixed', 'emotions', ' ', 'disapproval', 'of', 'Justine', 'combined', 'with', 'a', 'tinge', 'of', 'understanding', 'for', 'her', 'actions', ' '] 24\n",
            "after ['out', 'of', 'The', 'Good', 'Girl', 'with', 'mixed', 'emotions', ' ', 'disapproval', 'of', 'of', 'understanding', 'her']\n",
            "before [' ', ' ', 'Frailty', ' ', 'has', 'been', 'written', 'so', 'well', ' ', 'that', 'even', 'a', 'simple', ' ', ' ', 'Goddammit', ' ', ' '] 19\n",
            "after [' ', ' ', 'Frailty', 'Goddammit', 'even', 'been', 'simple', 'so', 'well', ' ', 'that', 'has', 'a', 'written', ' ', ' ', ' ', ' ', ' ']\n",
            "before ['Grenier', 'is', 'terrific', ' ', 'bringing', 'an', 'unforced', ' ', 'rapid', ' ', 'fire', 'delivery', 'to', 'Toback', ' s', 'Heidegger', ' ', 'and', 'Nietzsche', ' ', 'referencing', 'dialogue', ' '] 23\n",
            "after ['rapid', 'is', 'terrific', 'Grenier', 'bringing', 'an', 'unforced', ' ', 'dialogue', 'referencing', 'fire', 'delivery', ' ', 'Toback', ' s', 'Heidegger', ' ', 'and', 'Nietzsche', ' ', ' ', ' ', 'to']\n",
            "before ['The', 'Sundance', 'Film', 'Festival', 'has', 'become', 'so', 'buzz', ' ', 'obsessed', 'that', 'fans', 'and', 'producers', 'descend', 'upon', 'Utah', 'each', 'January', 'to', 'ferret', 'out', 'The', 'Next', 'Great', 'Thing', ' '] 27\n",
            "after ['The', 'Sundance', 'has', 'become', 'so', 'buzz', 'obsessed', 'that', 'and', 'descend', 'Utah', 'to', 'out', 'Next', 'Great']\n",
            "before [' ', 'Tadpole', ' ', 'was', 'one', 'of', 'the', 'films', 'so', 'declared', 'this', 'year', ' ', 'but', 'it', ' s', 'really', 'more', 'of', 'The', 'Next', 'Pretty', 'Good', 'Thing', ' '] 25\n",
            "after [' ', 'was', 'one', 'of', 'the', 'films', 'so', 'year', ' ', 'it', ' s', 'of', 'The', 'Next']\n",
            "before ['They', 'are', 'what', 'makes', 'it', 'worth', 'the', 'trip', 'to', 'the', 'theatre', ' '] 12\n",
            "They make it worth a theater trip \n",
            "after ['They', 'make', 'it', 'worth', 'a', 'theater', 'trip', '']\n",
            "before [' ', 'Taymor', ' ', 'utilizes', 'the', 'idea', 'of', 'making', 'Kahlo', ' s', 'art', 'a', 'living', ' ', 'breathing', 'part', 'of', 'the', 'movie', ' ', 'often', 'catapulting', 'the', 'artist', 'into', 'her', 'own', 'work', ' '] 29\n",
            "Taymor uses the idea of making Kahlo's art a living breath part of a film that often launches the artist into her own work \n",
            "after ['Taymor', 'uses', 'the', 'idea', 'of', 'making', \"Kahlo's\", 'art', 'a', 'living', 'breath', 'part', 'of', 'a', 'film', 'that', 'often', 'launches', 'the', 'artist', 'into', 'her', 'own', 'work', '']\n",
            "before [' ', ' ', ' ', 'Take', 'Care', 'of', 'My', 'Cat', ' ', ' ', 'is', 'an', 'honestly', 'nice', 'little', 'film', 'that', 'takes', 'us', 'on', 'an', 'examination', 'of', 'young', 'adult', 'life', 'in', 'urban', 'South', 'Korea', 'through', 'the', 'hearts', 'and', 'minds', 'of', 'the', 'five', 'principals', ' '] 40\n",
            "after [' ', ' ', ' ', 'Take', 'Care', ' ', 'My', 'Cat', 'through', 'of', 'is', 'an', 'the', 'us', 'little', 'film', 'that', 'takes', 'nice', 'on', 'an', 'examination', 'of', 'young', ' ', 'life', 'in', 'urban', 'South', 'Korea', 'adult', 'the', 'hearts', 'and', 'minds', 'of', 'honestly', 'five', 'principals', ' ']\n",
            "before ['It', ' s', 'not', 'life', ' ', 'affirming', ' ', 'its', 'vulgar', 'and', 'mean', ' ', 'but', 'I', 'liked', 'it', ' '] 17\n",
            "after [' ', 'I', 'not', 'it', 'life', 'affirming', ' ', 'its', 'vulgar', 'liked', 'mean', ' ', 'but', ' s', 'and', 'It', ' ']\n",
            "before ['What', ' ', ' ', 'Empire', ' ', 'lacks', 'in', 'depth', 'it', 'makes', 'up', 'for', 'with', 'its', 'heart', ' '] 16\n",
            "after ['What', ' ', ' ', 'lacks', 'in', 'depth', 'makes', 'with', 'its', ' ']\n",
            "before ['Much', 'of', 'the', 'movie', ' s', 'charm', 'lies', 'in', 'the', 'utter', 'cuteness', 'of', 'Stuart', 'and', 'Margolo', ' '] 16\n",
            "Much of the film's charm lies in the complete beauty of Stuart and Margolo \n",
            "after ['Much', 'of', 'the', \"film's\", 'charm', 'lies', 'in', 'the', 'complete', 'beauty', 'of', 'Stuart', 'and', 'Margolo', '']\n",
            "before [' ', ' ', '13', 'Conversations', ' ', 'holds', 'its', 'goodwill', 'close', ' ', 'but', 'is', 'relatively', 'slow', 'to', 'come', 'to', 'the', 'point', ' '] 20\n",
            "after [' ', '13', 'Conversations', ' ', 'holds', 'its', 'goodwill', 'close', ' ', 'is', 'to', 'to', 'the', 'point']\n",
            "before [' ', ' ', 'Auto', 'Focus', ' ', 'works', 'as', 'an', 'unusual', 'biopic', 'and', 'document', 'of', 'male', 'swingers', 'in', 'the', 'Playboy', 'era'] 19\n",
            "after ['Auto', 'Focus', 'works', 'as', 'unusual', 'biopic', 'and', 'male', 'in', 'era']\n",
            "before ['If', 'Mr ', 'Zhang', ' s', 'subject', 'matter', 'is', ' ', 'to', 'some', 'degree', 'at', 'least', ' ', 'quintessentially', 'American', ' ', 'his', 'approach', 'to', 'storytelling', 'might', 'be', 'called', 'Iranian', ' '] 26\n",
            "If the subject of mr. Zhang is to some extent at least characteristically American, his approach to storytelling can be called Iranian. \n",
            "after ['If', 'the', 'subject', 'of', 'mr.', 'Zhang', 'is', 'to', 'some', 'extent', 'at', 'least', 'characteristically', 'American,', 'his', 'approach', 'to', 'storytelling', 'can', 'be', 'called', 'Iranian.', '']\n",
            "before [' ', ' ', 'Cremaster', '3', ' ', 'should', 'come', 'with', 'the', 'warning', ' ', ' ', 'For', 'serious', 'film', 'buffs', 'only', ' ', ' '] 19\n",
            "after ['buffs', ' ', 'Cremaster', '3', ' ', 'only', 'come', 'with', ' ', 'warning', ' ', 'the', 'serious', 'For', ' ', ' ', 'should', ' ', 'film']\n",
            "before ['Made', 'me', 'unintentionally', 'famous', ' ', 'as', 'the', 'queasy', ' ', 'stomached', 'critic', 'who', 'staggered', 'from', 'the', 'theater', 'and', 'blacked', 'out', 'in', 'the', 'lobby', ' '] 23\n",
            "after ['Made', 'me', 'unintentionally', 'staggered', ' ', 'as', 'blacked', 'queasy', ' ', 'stomached', 'famous', 'theater', 'critic', 'from', 'the', 'out', 'and', 'the', 'who', 'in', 'the', 'lobby', ' ']\n",
            "before ['But', 'believe', 'it', 'or', 'not', ' ', 'it', ' s', 'one', 'of', 'the', 'most', 'beautiful', ' ', 'evocative', 'works', 'I', ' ', 've', 'seen', ' '] 21\n",
            "after ['But', 'believe', 'not', 'or', 'it', ' ', 'it', ' s', 'one', 'of', 'I', ' ', 've', ' ', 'evocative', 'works', 'the', 'beautiful', 'most', 'seen', ' ']\n",
            "before [' ', 'City', ' ', 'reminds', 'us', 'how', 'realistically', 'nuanced', 'a', 'Robert', 'De', 'Niro', 'performance', 'can', 'be', 'when', 'he', 'is', 'not', 'more', 'lucratively', 'engaged', 'in', 'the', 'shameless', 'self', ' ', 'caricature', 'of', ' ', 'Analyze', 'This', ' ', ' ', '1999', ' ', 'and', ' ', 'Analyze', 'That', ' ', ' ', 'promised', ' ', 'or', 'threatened', ' ', 'for', 'later', 'this', 'year', ' '] 52\n",
            "after [' ', 'City', ' ', 'reminds', ' ', 'how', 'caricature', 'nuanced', 'a', 'Robert', 'De', 'Niro', 'performance', 'can', 'be', 'when', 'he', 'is', 'not', 'more', 'lucratively', 'engaged', 'in', 'the', 'this', 'self', 'realistically', ' ', 'of', 'Analyze', 'Analyze', 'This', ' ', ' ', '1999', ' ', 'and', ' ', ' ', 'That', ' ', 'us', 'promised', ' ', 'or', 'threatened', ' ', 'for', 'later', 'shameless', 'year', ' ']\n",
            "before ['The', 'wanton', 'slipperiness', 'of', ' ', 'Corpus', 'and', 'its', 'amiable', 'jerking', 'and', 'reshaping', 'of', 'physical', 'time', 'and', 'space', 'would', 'make', 'it', 'a', 'great', 'piece', 'to', 'watch', 'with', 'kids', 'and', 'use', 'to', 'introduce', 'video', 'as', 'art', ' '] 35\n",
            "The muddy smoothness of the corpus and the amiable jolts and the redesign of physical time and space will make it a great creation to watch with children and use video presentation as art. \n",
            "after ['The', 'muddy', 'smoothness', 'of', 'the', 'corpus', 'and', 'the', 'amiable', 'jolts', 'and', 'the', 'redesign', 'of', 'physical', 'time', 'and', 'space', 'will', 'make', 'it', 'a', 'great', 'creation', 'to', 'watch', 'with', 'children', 'and', 'use', 'video', 'presentation', 'as', 'art.', '']\n",
            "before [' ', ' ', 'Frailty', ' ', 'starts', 'out', 'like', 'a', 'typical', 'Bible', 'killer', 'story', ' ', 'but', 'it', 'turns', 'out', 'to', 'be', 'significantly', 'different', ' ', 'and', 'better', ' ', 'than', 'most', 'films', 'with', 'this', 'theme', ' '] 32\n",
            "after [' ', 'Frailty', ' ', 'like', ' ', 'but', 'turns', 'to', 'significantly', 'different', ' ', 'most', 'theme']\n",
            "before ['A', 'crisp', 'psychological', 'drama', ' ', 'and', ' ', 'a', 'fascinating', 'little', 'thriller', 'that', 'would', 'have', 'been', 'perfect', 'for', 'an', 'old', ' ', ' ', 'Twilight', 'Zone', ' ', 'episode', ' '] 26\n",
            "after ['that', 'psychological', 'crisp', ' ', 'drama', 'and', ' ', 'a', 'fascinating', 'old', 'Zone', 'A', 'would', 'have', 'been', 'perfect', 'for', 'an', 'little', ' ', ' ', 'Twilight', 'thriller', ' ', 'episode', ' ']\n",
            "before ['It', 'has', 'more', 'than', 'a', 'few', 'moments', 'that', 'are', 'insightful', 'enough', 'to', 'be', 'fondly', 'remembered', 'in', 'the', 'endlessly', 'challenging', 'maze', 'of', 'moviegoing', ' '] 23\n",
            "after ['has', 'than', 'that', 'insightful', 'enough', 'to', 'fondly', 'endlessly', 'of']\n",
            "before ['It', ' s', 'only', 'in', 'fairy', 'tales', 'that', 'princesses', 'that', 'are', 'married', 'for', 'political', 'reason', 'live', 'happily', 'ever', 'after', ' '] 19\n",
            "after ['only', 'in', 'fairy', 'tales', 'that', 'that', 'married', 'political', 'happily', 'after']\n",
            "before [' ', ' ', 'Birthday', 'Girl', ' ', 'is', 'an', 'actor', ' s', 'movie', 'first', 'and', 'foremost', ' '] 14\n",
            "Birthday Girl is an art movie first of all \n",
            "after ['Birthday', 'Girl', 'is', 'an', 'art', 'movie', 'first', 'of', 'all', '']\n",
            "before ['I', 'walked', 'away', 'from', 'this', 'new', 'version', 'of', 'E T ', 'just', 'as', 'I', 'hoped', 'I', 'would', ' ', 'with', 'moist', 'eyes', ' '] 20\n",
            "after ['I', 'of', 'just', 'as', 'I', 'with', 'eyes']\n",
            "before ['This', 'version', 'moves', 'beyond', 'the', 'original', ' s', 'nostalgia', 'for', 'the', 'communal', 'film', 'experiences', 'of', 'yesteryear', 'to', 'a', 'deeper', 'realization', 'of', 'cinema', ' s', 'inability', 'to', 'stand', 'in', 'for', 'true', ' ', 'lived', 'experience', ' '] 32\n",
            "This version moves beyond the original nostalgia for the common cinematic experiences of the times to a deeper realization of the inability of cinema to sustain a true lived experience. \n",
            "after ['This', 'version', 'moves', 'beyond', 'the', 'original', 'nostalgia', 'for', 'the', 'common', 'cinematic', 'experiences', 'of', 'the', 'times', 'to', 'a', 'deeper', 'realization', 'of', 'the', 'inability', 'of', 'cinema', 'to', 'sustain', 'a', 'true', 'lived', 'experience.', '']\n",
            "before ['Mention', ' ', ' ', 'Solaris', ' ', 'five', 'years', 'from', 'now', 'and', 'I', ' ', 'm', 'sure', 'those', 'who', 'saw', 'it', 'will', 'have', 'an', 'opinion', 'to', 'share', ' '] 25\n",
            "Mention Solaris in five years and I’m sure those who have seen it will have their say \n",
            "after ['Mention', 'Solaris', 'in', 'five', 'years', 'and', 'I’m', 'sure', 'those', 'who', 'have', 'seen', 'it', 'will', 'have', 'their', 'say', '']\n",
            "before ['Ms ', 'Seigner', 'and', 'Mr ', 'Serrault', 'bring', 'fresh', ' ', 'unforced', 'naturalism', 'to', 'their', 'characters', ' '] 14\n",
            "Senier and Serow bring fresh and unreasonable naturalism to their characters \n",
            "after ['Senier', 'and', 'Serow', 'bring', 'fresh', 'and', 'unreasonable', 'naturalism', 'to', 'their', 'characters', '']\n",
            "before ['Allen', 'shows', 'he', 'can', 'outgag', 'any', 'of', 'those', 'young', 'whippersnappers', 'making', 'moving', 'pictures', 'today', ' '] 15\n",
            "Allen shows he can beat any of these young whippersnappers who make cartoons today \n",
            "after ['Allen', 'shows', 'he', 'can', 'beat', 'any', 'of', 'these', 'young', 'whippersnappers', 'who', 'make', 'cartoons', 'today', '']\n",
            "before ['A', 'good', 'film', 'with', 'a', 'solid', 'pedigree', 'both', 'in', 'front', 'of', 'and', ' ', 'more', 'specifically', ' ', 'behind', 'the', 'camera', ' '] 20\n",
            "after ['good', 'a', 'solid', 'pedigree', 'front', 'more', ' ', 'behind', ' ']\n",
            "before ['By', 'no', 'means', 'a', 'slam', ' ', 'dunk', 'and', 'sure', 'to', 'ultimately', 'disappoint', 'the', 'action', 'fans', 'who', 'will', 'be', 'moved', 'to', 'the', 'edge', 'of', 'their', 'seats', 'by', 'the', 'dynamic', 'first', 'act', ' ', 'it', 'still', 'comes', 'off', 'as', 'a', 'touching', ' ', 'transcendent', 'love', 'story', ' '] 43\n",
            "after ['By', 'no', 'a', 'slam', ' ', 'dunk', 'sure', 'to', 'ultimately', 'fans', 'be', 'to', 'edge', 'by', 'dynamic', 'act', 'comes', 'off', 'as', ' ', 'love', ' ']\n",
            "before ['I', 'encourage', 'young', 'and', 'old', 'alike', 'to', 'go', 'see', 'this', 'unique', 'and', 'entertaining', 'twist', 'on', 'the', 'classic', 'whale', ' s', 'tale', ' ', 'you', 'wo', 'n t', 'be', 'sorry', ' '] 27\n",
            "I encourage young and old to watch this unique and fun twist in a classic whale tale that you will not regret \n",
            "after ['I', 'encourage', 'young', 'and', 'old', 'to', 'watch', 'this', 'unique', 'and', 'fun', 'twist', 'in', 'a', 'classic', 'whale', 'tale', 'that', 'you', 'will', 'not', 'regret', '']\n",
            "before ['High', 'Crimes', 'steals', 'so', 'freely', 'from', 'other', 'movies', 'and', 'combines', 'enough', 'disparate', 'types', 'of', 'films', 'that', 'it', 'ca', 'n t', 'help', 'but', 'engage', 'an', 'audience', ' '] 25\n",
            "after ['High', 'Crimes', 'help', 'and', 'an', 'ca', 'other', 'movies', 'so', 'of', 'enough', 'disparate', 'types', 'combines', 'films', 'that', 'it', 'from', 'n t', 'steals', 'but', 'engage', 'freely', 'audience', ' ']\n",
            "before ['If', 'you', ' re', 'a', 'fan', 'of', 'the', 'series', 'you', ' ll', 'love', 'it', 'and', 'probably', 'want', 'to', 'see', 'it', 'twice', ' '] 20\n",
            "after ['If', ' re', 'a', 'of', 'you', ' ll', 'it', 'and', 'to', 'it']\n",
            "before ['It', 'celebrates', 'the', 'group', ' s', 'playful', 'spark', 'of', 'nonconformity', ' ', 'glancing', 'vividly', 'back', 'at', 'what', 'Hibiscus', 'grandly', 'called', 'his', ' ', 'angels', 'of', 'light', ' ', ' '] 25\n",
            "after ['the', ' s', 'playful', 'back', 'Hibiscus', 'light', ' ']\n",
            "before ['The', 'story', ' ', 'is', 'inspiring', ' ', 'ironic', ' ', 'and', 'revelatory', 'of', 'just', 'how', 'ridiculous', 'and', 'money', ' ', 'oriented', 'the', 'record', 'industry', 'really', 'is', ' '] 24\n",
            "after ['The', 'story', ' ', 'ironic', ' ', 'revelatory', 'just', 'how', 'ridiculous', 'money', 'oriented', 'the', 'record', 'is', ' ']\n",
            "before ['It', 'is', 'also', 'a', 'testament', 'to', 'the', 'integrity', 'and', 'vision', 'of', 'the', 'band', ' '] 14\n",
            "after ['It', 'integrity', 'and', 'vision', 'the', ' ']\n",
            "before ['Huppert', ' s', 'show', 'to', 'steal', 'and', 'she', 'makes', 'a', 'meal', 'of', 'it', ' ', 'channeling', 'Kathy', 'Baker', ' s', 'creepy', 'turn', 'as', 'the', 'repressed', 'mother', 'on', 'Boston', 'Public', 'just', 'as', 'much', 'as', '8', 'Women', ' s', 'Augustine', ' '] 35\n",
            "after ['Huppert', 'just', 'show', 'to', '8', 'and', 'she', 'makes', 'a', ' s', 'of', 'it', ' ', 'Boston', 'Kathy', 'Baker', ' s', 'creepy', 'turn', 'as', 'the', 'repressed', 'mother', 'on', 'channeling', 'Public', 'meal', 'as', 'much', 'as', 'steal', 'Women', ' s', 'Augustine', ' ']\n",
            "before ['One', 'of', 'the', 'best', 'silly', 'horror', 'movies', 'of', 'recent', 'memory', ' ', 'with', 'some', 'real', 'shocks', 'in', 'store', 'for', 'unwary', 'viewers', ' '] 21\n",
            "after ['the', 'horror', 'memory', ' ', 'with', 'some', ' ']\n",
            "before ['Jeffrey', 'Tambor', ' s', 'performance', 'as', 'the', 'intelligent', 'jazz', ' ', 'playing', 'exterminator', 'is', 'Oscar', ' ', 'worthy', ' '] 16\n",
            "Jeffrey Tambor's performance as the intelligent jazz exterminator is Oscar-worthy \n",
            "after ['Jeffrey', \"Tambor's\", 'performance', 'as', 'the', 'intelligent', 'jazz', 'exterminator', 'is', 'Oscar-worthy', '']\n",
            "before ['Stevens', ' ', 'vibrant', 'creative', 'instincts', 'are', 'the', 'difference', 'between', 'this', 'and', 'countless', 'other', 'flicks', 'about', 'guys', 'and', 'dolls', ' '] 19\n",
            "after ['Stevens', ' ', 'vibrant', 'creative', 'difference', 'between', 'and', 'flicks', 'about', 'dolls', ' ']\n",
            "before ['that', 'it', ' ll', 'probably', 'be', 'the', 'best', 'and', 'most', 'mature', 'comedy', 'of', 'the', '2002', 'summer', 'season', 'speaks', 'more', 'of', 'the', 'season', 'than', 'the', 'picture'] 24\n",
            "after ['of', 'more', ' ll', 'probably', 'be', 'the', 'best', 'speaks', 'most', 'mature', 'comedy', 'that', 'the', '2002', 'summer', 'the', 'and', 'it', 'of', 'picture', 'season', 'than', 'season', 'the']\n",
            "before ['Meyjes', ' ', 'provocative', 'film', 'might', 'be', 'called', 'an', 'example', 'of', 'the', 'haphazardness', 'of', 'evil', ' '] 15\n",
            "after ['Meyjes', ' ', 'provocative', 'might', 'called', 'an', 'haphazardness', 'of', 'evil', ' ']\n",
            "before ['Tian', 'emphasizes', 'the', 'isolation', 'of', 'these', 'characters', 'by', 'confining', 'color', 'to', 'Liyan', ' s', 'backyard', ' '] 15\n",
            "after ['Tian', 'characters', 'these', 'the', 'of', 'backyard', 'Liyan', 'by', 'confining', 'color', 'to', 'emphasizes', ' s', 'isolation', ' ']\n",
            "before ['Too', 'damn', 'weird', 'to', 'pass', 'up', ' ', 'and', 'for', 'the', 'blacklight', 'crowd', ' ', 'way', 'cheaper', ' ', 'and', 'better', ' ', 'than', 'Pink', 'Floyd', 'tickets', ' '] 24\n",
            "after ['Too', 'damn', 'Floyd', 'to', 'pass', 'up', 'way', ' ', 'for', 'the', 'blacklight', 'crowd', ' ', 'and', 'cheaper', 'Pink', 'and', 'better', ' ', 'than', ' ', 'weird', 'tickets', ' ']\n",
            "before ['But', 'if', 'you', ' ', 've', 'paid', 'a', 'matinee', 'price', 'and', 'bought', 'a', 'big', 'tub', 'of', 'popcorn', ' ', 'there', ' s', 'guilty', 'fun', 'to', 'be', 'had', 'here', ' '] 26\n",
            "after ['you', 'price', 'But', ' ', ' s', 'paid', 'a', 'matinee', 'if', 'of', 'bought', 'a', 'big', 'be', 'and', 'popcorn', ' ', 'there', 've', 'guilty', 'fun', 'to', 'tub', 'had', 'here', ' ']\n",
            "before ['The', 'Grey', 'Zone', 'gives', 'voice', 'to', 'a', 'story', 'that', 'needs', 'to', 'be', 'heard', 'in', 'the', 'sea', 'of', 'Holocaust', 'movies', ' ', 'but', 'the', 'film', 'suffers', 'from', 'its', 'own', 'difficulties', ' '] 29\n",
            "Zone Zone provides a storyline that needs to be heard in the sea of Holocaust movies but the film suffers from its own difficulties \n",
            "after ['Zone', 'Zone', 'provides', 'a', 'storyline', 'that', 'needs', 'to', 'be', 'heard', 'in', 'the', 'sea', 'of', 'Holocaust', 'movies', 'but', 'the', 'film', 'suffers', 'from', 'its', 'own', 'difficulties', '']\n",
            "before ['It', ' s', 'a', 'sweet', ' ', 'laugh', ' ', 'a', ' ', 'minute', 'crowd', 'pleaser', 'that', 'lifts', 'your', 'spirits', 'as', 'well', 'as', 'the', 'corners', 'of', 'your', 'mouth', ' '] 25\n",
            "after ['It', ' s', 'a', 'sweet', ' ', 'lifts', 'the', 'a', ' ', 'minute', 'as', 'pleaser', 'that', 'of', 'your', 'spirits', 'corners', 'well', 'as', ' ', 'crowd', 'laugh', 'your', 'mouth', ' ']\n",
            "before ['It', ' s', 'tough', 'to', 'watch', ' ', 'but', 'it', ' s', 'a', 'fantastic', 'movie', ' '] 13\n",
            "after ['fantastic', ' ', 'tough', 'to', 'watch', ' s', 'but', 'it', 'movie', ' s', 'a', 'It', ' ']\n",
            "before ['The', 'best', 'animated', 'feature', 'to', 'hit', 'theaters', 'since', 'Beauty', 'and', 'the', 'Beast', '11', 'years', 'ago', ' '] 16\n",
            "after ['The', 'best', 'Beauty', 'feature', '11', 'the', 'theaters', 'since', 'animated', 'and', 'hit', 'Beast', 'to', 'years', 'ago', ' ']\n",
            "before ['What', 'saves', 'this', 'deeply', 'affecting', 'film', 'from', 'being', 'merely', 'a', 'collection', 'of', 'wrenching', 'cases', 'is', 'Corcuera', ' s', 'attention', 'to', 'detail', ' '] 21\n",
            "What saves this film deeply affected by being merely a collection of wrenching cases is Corcuera’s attention to detail \n",
            "after ['What', 'saves', 'this', 'film', 'deeply', 'affected', 'by', 'being', 'merely', 'a', 'collection', 'of', 'wrenching', 'cases', 'is', 'Corcuera’s', 'attention', 'to', 'detail', '']\n",
            "before ['So', 'purely', 'enjoyable', 'that', 'you', 'might', 'not', 'even', 'notice', 'it', ' s', 'a', 'fairly', 'straightforward', 'remake', 'of', 'Hollywood', 'comedies', 'such', 'as', 'Father', 'of', 'the', 'Bride', ' '] 25\n",
            "after ['So', 'that', 'might', 'not', 'even', 'notice', 'it', ' s', 'straightforward', 'of', 'comedies', 'Father', 'Bride', ' ']\n",
            "before ['If', 'you', ' re', 'willing', 'to', 'have', 'fun', 'with', 'it', ' ', 'you', 'wo', 'n t', 'feel', 'cheated', 'by', 'the', 'high', 'infidelity', 'of', 'Unfaithful', ' '] 22\n",
            "after ['you', ' re', 'willing', 'to', 'fun', 'with', 'it', ' ', 'you', 'feel', 'the', 'infidelity']\n",
            "before ['An', 'enthralling', 'aesthetic', 'experience', ' ', 'one', 'that', ' s', 'steeped', 'in', 'mystery', 'and', 'a', 'ravishing', ' ', 'baroque', 'beauty', ' '] 18\n",
            "after ['An', 'enthralling', 'aesthetic', 'one', 'that', 'steeped', 'a', 'baroque', ' ']\n",
            "before ['The', 'quirky', 'drama', 'touches', 'the', 'heart', 'and', 'the', 'funnybone', 'thanks', 'to', 'the', 'energetic', 'and', 'always', 'surprising', 'performance', 'by', 'Rachel', 'Griffiths', ' '] 21\n",
            "after ['The', 'quirky', 'thanks', 'touches', 'the', 'by', 'and', 'the', 'funnybone', 'drama', 'performance', 'energetic', 'the', 'and', 'always', 'to', 'surprising', 'heart', 'Rachel', 'Griffiths', ' ']\n",
            "before ['A', 'captivating', 'coming', ' ', 'of', ' ', 'age', 'story', 'that', 'may', 'also', 'be', 'the', 'first', 'narrative', 'film', 'to', 'be', 'truly', 'informed', 'by', 'the', 'wireless', 'age', ' '] 25\n",
            "A fascinating and engaging story to reach the age that is perhaps the first story film to really be aware of the wireless age \n",
            "after ['A', 'fascinating', 'and', 'engaging', 'story', 'to', 'reach', 'the', 'age', 'that', 'is', 'perhaps', 'the', 'first', 'story', 'film', 'to', 'really', 'be', 'aware', 'of', 'the', 'wireless', 'age', '']\n",
            "before ['You', ' d', 'have', 'to', 'be', 'a', 'most', 'hard', ' ', 'hearted', 'person', 'not', 'to', 'be', 'moved', 'by', 'this', 'drama', ' '] 19\n",
            "You have to be a hard-hearted person to not be touched by this drama \n",
            "after ['You', 'have', 'to', 'be', 'a', 'hard-hearted', 'person', 'to', 'not', 'be', 'touched', 'by', 'this', 'drama', '']\n",
            "before ['Allen', ' s', 'underestimated', 'charm', 'delivers', 'more', 'goodies', 'than', 'lumps', 'of', 'coal', ' '] 12\n",
            "after ['more', 'lumps', 'delivers', 'charm', 'Allen', 'underestimated', ' ', 'than', ' s', 'of', 'coal', 'goodies']\n",
            "before ['The', 'picture', 'uses', 'humor', 'and', 'a', 'heartfelt', 'conviction', 'to', 'tell', 'a', 'story', 'about', 'discovering', 'your', 'destination', 'in', 'life', ' ', 'but', 'also', 'acknowledging', 'the', 'places', ' ', 'and', 'the', 'people', ' ', 'from', 'whence', 'you', 'came', ' '] 34\n",
            "The image uses humor and sincere belief to tell you about finding your purpose in life but also to claim a place with people from where you come from \n",
            "after ['The', 'image', 'uses', 'humor', 'and', 'sincere', 'belief', 'to', 'tell', 'you', 'about', 'finding', 'your', 'purpose', 'in', 'life', 'but', 'also', 'to', 'claim', 'a', 'place', 'with', 'people', 'from', 'where', 'you', 'come', 'from', '']\n",
            "before ['This', 'version', 'does', 'justice', 'both', 'to', 'Stevenson', 'and', 'to', 'the', 'sci', ' ', 'fi', 'genre', ' '] 15\n",
            "This version does justice both to Stevenson and to the sci   fi genre \n",
            "after ['This', 'version', 'does', 'justice', 'both', 'to', 'Stevenson', 'and', 'to', 'the', 'sci', '', '', 'fi', 'genre', '']\n",
            "before ['Cho', ' s', 'latest', 'comic', 'set', 'is', 'n t', 'as', 'sharp', 'or', 'as', 'fresh', 'as', 'I', ' ', 'm', 'the', 'One', 'That', 'I', 'Want', ' ', 'but', 'it', ' s', 'still', 'damn', 'funny', 'stuff', ' '] 30\n",
            "after ['fresh', ' s', 'latest', 'comic', 'set', 'is', 'still', 'as', 'sharp', 'or', 'as', ' ', 'as', 'I', ' ', 'm', 'the', 'One', 'That', 'I', 'Want', ' ', 'but', 'it', ' s', 'n t', 'damn', 'funny', 'stuff', 'Cho']\n",
            "before ['In', 'The', 'Pianist', ' ', 'Polanski', 'is', 'saying', 'what', 'he', 'has', 'long', 'wanted', 'to', 'say', ' ', 'confronting', 'the', 'roots', 'of', 'his', 'own', 'preoccupations', 'and', 'obsessions', ' ', 'and', 'he', 'allows', 'nothing', 'to', 'get', 'in', 'the', 'way', ' '] 35\n",
            "after ['what', 'The', 'Pianist', ' ', 'wanted', 'confronting', 'saying', 'In', 'his', 'has', 'long', 'Polanski', 'to', 'say', ' ', 'is', 'the', 'roots', 'of', 'own', 'he', 'preoccupations', 'and', 'obsessions', ' ', 'and', 'he', 'allows', 'nothing', 'to', 'get', 'in', 'the', 'way', ' ']\n",
            "before ['Despite', 'the', 'film', ' s', 'shortcomings', ' ', 'the', 'stories', 'are', 'quietly', 'moving', ' '] 12\n",
            "Despite the film's shortcomings, the stories are quiet \n",
            "after ['Despite', 'the', \"film's\", 'shortcomings,', 'the', 'stories', 'are', 'quiet', '']\n",
            "before ['It', ' s', 'the', 'filmmakers', ' ', 'post', ' ', 'camp', 'comprehension', 'of', 'what', 'made', 'old', ' ', 'time', 'B', 'movies', 'good', ' ', 'bad', 'that', 'makes', 'Eight', 'Legged', 'Freaks', 'a', 'perfectly', 'entertaining', 'summer', 'diversion', ' '] 31\n",
            "after ['made', 'makes', 'the', 'filmmakers', ' ', 'post', ' ', 'camp', 'comprehension', 'of', 'what', ' ', 'old', 'It', 'time', 'B', 'movies', 'good', 'Eight', 'bad', 'that', ' s', 'Freaks', 'Legged', ' ', 'a', 'perfectly', 'entertaining', 'summer', 'diversion', ' ']\n",
            "before [' ', 'best', 'seen', 'as', 'speculative', 'history', ' ', 'as', 'much', 'an', 'exploration', 'of', 'the', 'paranoid', 'impulse', 'as', 'a', 'creative', 'sequel', 'to', 'the', 'Warren', 'Report', ' '] 24\n",
            "after [' ', 'seen', 'as', ' ', 'as', 'much', 'exploration', 'the', 'impulse', 'creative', 'to', 'Report']\n",
            "before ['The', 'Saigon', 'of', '1952', 'is', 'an', 'uneasy', 'mix', 'of', 'sensual', 'delights', 'and', 'simmering', 'violence', ' ', 'and', 'The', 'Quiet', 'American', 'brings', 'us', 'right', 'into', 'the', 'center', 'of', 'that', 'world', ' '] 29\n",
            "The 1952 Saigon is an unmistakable mix of joy and simplicity, and The Quiet American brings us to the heart of that world. \n",
            "after ['The', '1952', 'Saigon', 'is', 'an', 'unmistakable', 'mix', 'of', 'joy', 'and', 'simplicity,', 'and', 'The', 'Quiet', 'American', 'brings', 'us', 'to', 'the', 'heart', 'of', 'that', 'world.', '']\n",
            "before ['With', 'Dirty', 'Deeds', ' ', 'David', 'Caesar', 'has', 'stepped', 'into', 'the', 'mainstream', 'of', 'filmmaking', 'with', 'an', 'assurance', 'worthy', 'of', 'international', 'acclaim', 'and', 'with', 'every', 'cinematic', 'tool', 'well', 'under', 'his', 'control', ' ', 'driven', 'by', 'a', 'natural', 'sense', 'for', 'what', 'works', 'on', 'screen', ' '] 41\n",
            "after ['With', 'Dirty', ' ', 'Caesar', 'has', 'stepped', 'into', 'of', 'of', 'and', 'with', 'cinematic', 'tool', 'well', 'under', 'control', 'driven', 'by', 'a', 'sense', 'for', 'works', 'on', ' ']\n",
            "before ['Try', 'Hell', 'House', ' ', 'which', 'documents', 'the', 'cautionary', 'Christian', 'spook', ' ', 'a', ' ', 'rama', 'of', 'the', 'same', 'name', ' '] 19\n",
            "Try Hell House, which documents the Christian warning that scares a branch of the same name. \n",
            "after ['Try', 'Hell', 'House,', 'which', 'documents', 'the', 'Christian', 'warning', 'that', 'scares', 'a', 'branch', 'of', 'the', 'same', 'name.', '']\n",
            "before ['As', 'comedic', 'spotlights', 'go', ' ', 'Notorious', 'C H O ', 'hits', 'all', 'the', 'verbal', 'marks', 'it', 'should', ' '] 15\n",
            "after ['comedic', 'spotlights', ' ', 'Notorious', 'C H O ', 'all', 'the', 'marks', 'should']\n",
            "before ['Frida', 'is', 'n t', 'that', 'much', 'different', 'from', 'many', 'a', 'Hollywood', 'romance', ' '] 12\n",
            "after ['Frida', 'n t', 'that', 'much', 'many', 'a', 'romance']\n",
            "before ['What', 'sets', 'it', 'apart', 'is', 'the', 'vision', 'that', 'Taymor', ' ', 'the', 'avant', 'garde', 'director', 'of', 'Broadway', ' s', 'The', 'Lion', 'King', 'and', 'the', 'film', 'Titus', ' ', 'brings', ' '] 27\n",
            "It is distinguished by a vision brought by avant-garde director Taymor from Broadway’s The Lion King and the film Titus \n",
            "after ['It', 'is', 'distinguished', 'by', 'a', 'vision', 'brought', 'by', 'avant-garde', 'director', 'Taymor', 'from', 'Broadway’s', 'The', 'Lion', 'King', 'and', 'the', 'film', 'Titus', '']\n",
            "before ['Stevens', 'has', 'a', 'flair', 'for', 'dialogue', 'comedy', ' ', 'the', 'film', 'operates', 'nicely', 'off', 'the', 'element', 'of', 'surprise', ' ', 'and', 'the', 'large', 'cast', 'is', 'solid', ' '] 25\n",
            "after ['Stevens', 'a', 'for', ' ', 'the', 'the', 'element', 'of', 'surprise', 'the', 'large', 'solid']\n",
            "before ['It', ' s', 'good', ' ', 'hard', ' ', 'edged', 'stuff', ' ', 'violent', 'and', 'a', 'bit', 'exploitative', 'but', 'also', 'nicely', 'done', ' ', 'morally', 'alert', 'and', 'street', ' ', 'smart', ' '] 26\n",
            "This hard-working material is violent and slightly exploited but also morally alert and street smart. \n",
            "after ['This', 'hard-working', 'material', 'is', 'violent', 'and', 'slightly', 'exploited', 'but', 'also', 'morally', 'alert', 'and', 'street', 'smart.', '']\n",
            "before [' ', 'begins', 'with', 'promise', ' ', 'but', 'runs', 'aground', 'after', 'being', 'snared', 'in', 'its', 'own', 'tangled', 'plot', ' '] 17\n",
            "after ['begins', 'promise', ' ', 'after', 'being', 'snared', 'its', 'tangled']\n",
            "before ['A', 'reasonably', 'entertaining', 'sequel', 'to', '1994', ' s', 'surprise', 'family', 'hit', 'that', 'may', 'strain', 'adult', 'credibility', ' '] 16\n",
            "after ['A', 'to', 'entertaining', 'sequel', 'reasonably', '1994', ' s', 'family', 'that', 'credibility', 'surprise', 'may', 'hit', 'adult', 'strain', ' ']\n",
            "before ['a', 'confident', ' ', 'richly', 'acted', ' ', 'emotionally', 'devastating', 'piece', 'of', 'work', 'and', '2002', ' s', 'first', 'great', 'film'] 17\n",
            "after ['confident', 'acted', 'piece', 'and', 'great', 'film']\n",
            "before ['The', 'casting', 'of', 'von', 'Sydow', ' ', 'is', 'itself', 'Intacto', ' s', 'luckiest', 'stroke', ' '] 13\n",
            "after ['von', 'Sydow', ' ', 'itself', 'Intacto', ' s', 'luckiest', 'stroke']\n",
            "before ['No', ' ', 'it', ' s', 'not', 'as', 'single', ' ', 'minded', 'as', 'John', 'Carpenter', ' s', 'original', ' ', 'but', 'it', ' s', 'sure', 'a', 'lot', 'smarter', 'and', 'more', 'unnerving', 'than', 'the', 'sequels', ' '] 29\n",
            "after ['No', 'it', ' s', 'not', 'as', ' ', 'minded', 'as', 'John', 'but', ' s', 'sure', 'lot', 'smarter', 'and', 'more', 'unnerving', 'than', 'sequels']\n",
            "before ['A', 'gem', 'of', 'a', 'romantic', 'crime', 'comedy', 'that', 'turns', 'out', 'to', 'be', 'clever', ' ', 'amusing', 'and', 'unpredictable', ' '] 18\n",
            "after ['A', 'gem', 'of', 'romantic', 'turns', 'crime', 'comedy', 'that', 'a', 'out', 'to', 'be', 'and', ' ', 'amusing', 'clever', 'unpredictable', ' ']\n",
            "before ['Stands', 'as', 'one', 'of', 'the', 'year', ' s', 'most', 'intriguing', 'movie', 'experiences', ' ', 'letting', 'its', 'imagery', 'speak', 'for', 'it', 'while', 'it', 'forces', 'you', 'to', 'ponder', 'anew', 'what', 'a', 'movie', 'can', 'be', ' '] 31\n",
            "after ['a', 'speak', 'its', 'of', 'intriguing', 'year', ' s', 'most', 'the', 'movie', 'experiences', 'imagery', 'letting', 'one', ' ', 'as', 'for', 'it', 'while', 'it', 'forces', 'you', 'to', 'ponder', 'anew', 'what', 'Stands', 'movie', 'can', 'be', ' ']\n",
            "before ['There', ' s', 'a', 'part', 'of', 'us', 'that', 'can', 'not', 'help', 'be', 'entertained', 'by', 'the', 'sight', 'of', 'someone', 'getting', 'away', 'with', 'something', ' '] 22\n",
            "There are some of us who can't be entertained when someone gets away with something \n",
            "after ['There', 'are', 'some', 'of', 'us', 'who', \"can't\", 'be', 'entertained', 'when', 'someone', 'gets', 'away', 'with', 'something', '']\n",
            "before ['What', ' s', 'not', 'to', 'like', 'about', 'a', 'movie', 'with', 'a', ' ', 'children', ' s', ' ', 'song', 'that', 'includes', 'the', 'line', ' ', 'My', 'stepdad', ' s', 'not', 'mean', ' ', 'he', ' s', 'just', 'adjusting', ' ', ' '] 32\n",
            "after ['What', ' s', 'not', 'to', 'line', 'about', 'a', 'movie', 'with', 'a', ' ', 'children', ' ', ' ', 'song', 'that', 'includes', 'the', 'like', ' ', 'My', 'stepdad', ' ', 'not', 'mean', ' ', 'just', ' s', 'he', 'adjusting', ' s', ' s']\n",
            "before ['This', 'English', ' ', 'language', 'version', ' ', 'does', 'full', 'honor', 'to', 'Miyazaki', ' s', 'teeming', 'and', 'often', 'unsettling', 'landscape', ' ', 'and', 'to', 'the', 'conflicted', 'complexity', 'of', 'his', 'characters', ' '] 27\n",
            "after ['This', ' ', ' ', 'language', 'version', ' ', 'his', 'full', ' ', 'to', 'Miyazaki', ' s', 'teeming', 'and', 'often', 'to', 'landscape', 'English', 'conflicted', 'unsettling', 'the', 'and', 'complexity', 'of', 'does', 'characters', 'honor']\n",
            "before ['The', 'pleasures', 'that', 'it', 'does', 'afford', 'may', 'be', 'enough', 'to', 'keep', 'many', 'moviegoers', 'occupied', 'amidst', 'some', 'of', 'the', 'more', 'serious', ' ', 'minded', 'concerns', 'of', 'other', 'year', ' ', 'end', 'movies', ' '] 30\n",
            "The fun it does may be enough to give many fans a place in the more serious concerns of other year-end movies. \n",
            "after ['The', 'fun', 'it', 'does', 'may', 'be', 'enough', 'to', 'give', 'many', 'fans', 'a', 'place', 'in', 'the', 'more', 'serious', 'concerns', 'of', 'other', 'year-end', 'movies.', '']\n",
            "before ['A', 'brutally', 'honest', 'documentary', 'about', 'a', 'much', 'anticipated', 'family', 'reunion', 'that', 'goes', 'wrong', 'thanks', 'to', 'culture', 'shock', 'and', 'a', 'refusal', 'to', 'empathize', 'with', 'others', ' '] 25\n",
            "A cruel, honest documentary about the reunion of families who expect so much from the cultural shock and refusal to empathize with others. \n",
            "after ['A', 'cruel,', 'honest', 'documentary', 'about', 'the', 'reunion', 'of', 'families', 'who', 'expect', 'so', 'much', 'from', 'the', 'cultural', 'shock', 'and', 'refusal', 'to', 'empathize', 'with', 'others.', '']\n",
            "before [' ', 'Haynes', ' ', ' ', 'homage', 'to', 'such', 'films', 'as', ' ', ' ', 'All', 'That', 'Heaven', 'Allows', ' ', 'and', ' ', ' ', 'Imitation', 'of', 'Life', ' ', 'transcends', 'them', ' '] 26\n",
            "after [' ', 'Haynes', ' ', 'Life', 'homage', 'All', 'such', 'films', 'as', ' ', ' ', 'to', 'That', 'Heaven', 'Allows', ' ', 'and', ' ', ' ', 'Imitation', 'of', ' ', ' ', 'transcends', 'them', ' ']\n",
            "before ['Although', 'fairly', 'involving', 'as', 'far', 'as', 'it', 'goes', ' ', 'the', 'film', 'does', 'n t', 'end', 'up', 'having', 'much', 'that', 'is', 'fresh', 'to', 'say', 'about', 'growing', 'up', 'Catholic', 'or', ' ', 'really', ' ', 'anything', ' '] 32\n",
            "after ['involving', 'it', 'goes', ' ', 'the', 'n t', 'up', 'having', 'is', 'fresh', 'to', 'say', 'Catholic', ' ', 'really', ' ']\n",
            "before ['I', 'stopped', 'thinking', 'about', 'how', 'good', 'it', 'all', 'was', ' ', 'and', 'started', 'doing', 'nothing', 'but', 'reacting', 'to', 'it', ' ', 'feeling', 'a', 'part', 'of', 'its', 'grand', 'locations', ' ', 'thinking', 'urgently', 'as', 'the', 'protagonists', 'struggled', ' ', 'feeling', 'at', 'the', 'mercy', 'of', 'its', 'inventiveness', ' ', 'gasping', 'at', 'its', 'visual', 'delights', ' '] 48\n",
            "after ['I', 'stopped', 'thinking', ' ', 'how', 'good', 'it', 'all', 'was', ' ', 'and', 'started', 'gasping', 'its', 'but', 'mercy', 'to', 'it', ' ', 'feeling', 'a', 'part', 'of', 'nothing', 'grand', ' ', 'locations', 'thinking', 'urgently', 'as', 'the', 'protagonists', 'struggled', ' ', 'feeling', 'at', 'the', 'reacting', 'of', 'its', 'inventiveness', ' ', 'doing', 'at', 'its', 'visual', 'delights', 'about']\n",
            "before ['Probably', 'the', 'best', 'case', 'for', 'Christianity', 'since', 'Chesterton', 'and', 'Lewis', ' '] 11\n",
            "Probably the best case scenario for Christianity since Chesterton and Lewis \n",
            "after ['Probably', 'the', 'best', 'case', 'scenario', 'for', 'Christianity', 'since', 'Chesterton', 'and', 'Lewis', '']\n",
            "before ['A', 'gently', 'funny', ' ', 'sweetly', 'adventurous', 'film', 'that', 'makes', 'you', 'feel', 'genuinely', 'good', ' ', 'that', 'is', 'to', 'say', ' ', 'entirely', 'unconned', 'by', 'false', 'sentiment', 'or', 'sharp', ' ', 'overmanipulative', 'Hollywood', 'practices', ' '] 31\n",
            "after ['A', 'gently', 'genuinely', ' ', ' ', 'adventurous', 'film', 'that', 'makes', 'feel', 'you', 'funny', 'good', ' ', 'by', 'is', 'to', 'say', ' ', 'entirely', 'unconned', 'that', 'false', 'sentiment', 'or', 'sharp', 'sweetly', 'overmanipulative', 'Hollywood', 'practices', ' ']\n",
            "before ['While', 'some', 'will', 'object', 'to', 'the', 'idea', 'of', 'a', 'Vietnam', 'picture', 'with', 'such', 'a', 'rah', ' ', 'rah', ' ', 'patriotic', 'tone', ' ', 'Soldiers', 'ultimately', 'achieves', 'its', 'main', 'strategic', 'objective', ' ', 'dramatizing', 'the', 'human', 'cost', 'of', 'the', 'conflict', 'that', 'came', 'to', 'define', 'a', 'generation', ' '] 43\n",
            "after ['some', 'will', 'object', 'to', 'Vietnam', 'picture', 'with', 'rah', ' ', 'patriotic', 'tone', ' ', 'ultimately', 'achieves', 'its', 'objective', ' ', 'dramatizing', 'human', 'conflict', 'that', 'to', 'define', 'generation', ' ']\n",
            "before ['The', 'solid', 'filmmaking', 'and', 'convincing', 'characters', 'makes', 'this', 'a', 'high', 'water', 'mark', 'for', 'this', 'genre', ' '] 16\n",
            "after ['The', 'for', 'filmmaking', 'convincing', 'characters', 'and', 'makes', 'high', 'a', 'this', 'water', ' ', 'solid', 'this', 'genre', 'mark']\n",
            "before ['Seen', 'in', 'that', 'light', ' ', 'Moonlight', 'Mile', 'should', 'strike', 'a', 'nerve', 'in', 'many', ' '] 14\n",
            "Seen in that light, the Moon Mile should hit a nerve on many \n",
            "after ['Seen', 'in', 'that', 'light,', 'the', 'Moon', 'Mile', 'should', 'hit', 'a', 'nerve', 'on', 'many', '']\n",
            "before ['It', ' s', 'endlessly', 'inventive', ' ', 'consistently', 'intelligent', 'and', 'sickeningly', 'savage', ' '] 11\n",
            "after ['endlessly', ' ', 'consistently', 'intelligent', 'sickeningly', 'savage']\n",
            "before ['Far', 'From', 'Heaven', 'is', 'a', 'dazzling', 'conceptual', 'feat', ' ', 'but', 'more', 'than', 'that', ' ', 'it', ' s', 'a', 'work', 'of', 'enthralling', 'drama', ' '] 22\n",
            "Far From Heaven is a stunning conceptual feat, but more than that it is a captivating drama \n",
            "after ['Far', 'From', 'Heaven', 'is', 'a', 'stunning', 'conceptual', 'feat,', 'but', 'more', 'than', 'that', 'it', 'is', 'a', 'captivating', 'drama', '']\n",
            "before ['Affectionately', 'reminds', 'us', 'that', ' ', 'in', 'any', 'language', ' ', 'the', 'huge', 'stuff', 'in', 'life', 'can', 'usually', 'be', 'traced', 'back', 'to', 'the', 'little', 'things', ' '] 24\n",
            "after ['traced', 'reminds', 'us', 'that', 'be', 'to', 'usually', 'language', ' ', 'the', 'huge', 'stuff', 'in', 'life', 'back', 'any', ' ', 'Affectionately', 'can', 'in', 'the', 'little', 'things', ' ']\n",
            "before ['A', 'drama', 'of', 'great', 'power', ' ', 'yet', 'some', 'members', 'of', 'the', 'audience', 'will', 'leave', 'the', 'theater', 'believing', 'they', 'have', 'seen', 'a', 'comedy', ' '] 23\n",
            "A superpower drama but some audience will leave the stage thinking they have seen a comedy \n",
            "after ['A', 'superpower', 'drama', 'but', 'some', 'audience', 'will', 'leave', 'the', 'stage', 'thinking', 'they', 'have', 'seen', 'a', 'comedy', '']\n",
            "before ['A', 'story', 'about', 'intelligent', 'high', 'school', 'students', 'that', 'deals', 'with', 'first', 'love', 'sweetly', 'but', 'also', 'seriously', ' '] 17\n",
            "after ['about', 'love', 'A', 'intelligent', 'seriously', 'school', 'students', 'deals', 'that', 'with', 'story', 'first', 'sweetly', 'but', 'also', 'high', ' ']\n",
            "before ['Same', 'song', ' ', 'second', 'verse', ' ', 'coulda', 'been', 'better', ' ', 'but', 'it', 'coulda', 'been', 'worse', ' '] 16\n",
            "after ['Same', 'song', ' ', 'second', 'verse', 'coulda', 'been', 'coulda', 'better', 'been', 'but', 'it', ' ', ' ', 'worse', ' ']\n",
            "before ['It', ' s', 'a', 'technically', 'superb', 'film', ' ', 'shining', 'with', 'all', 'the', 'usual', 'Spielberg', 'flair', ' ', 'expertly', 'utilizing', 'the', 'talents', 'of', 'his', 'top', ' ', 'notch', 'creative', 'team', ' '] 27\n",
            "That is the best shine, and the talents of the technically excellent lot of players rutrum tristique Spielberg the birth, the pain of the top of the notch with all his keen mind than usual, \n",
            "after ['That', 'is', 'the', 'best', 'shine,', 'and', 'the', 'talents', 'of', 'the', 'technically', 'excellent', 'lot', 'of', 'players', 'rutrum', 'tristique', 'Spielberg', 'the', 'birth,', 'the', 'pain', 'of', 'the', 'top', 'of', 'the', 'notch', 'with', 'all', 'his', 'keen', 'mind', 'than', 'usual,', '']\n",
            "before [' ', 'there', 'is', 'enough', 'originality', 'in', ' ', 'Life', ' ', 'to', 'distance', 'it', 'from', 'the', 'pack', 'of', 'paint', ' ', 'by', ' ', 'number', 'romantic', 'comedies', 'that', 'so', 'often', 'end', 'up', 'on', 'cinema', 'screens', ' '] 32\n",
            "There is a perfect start in Life to distinguish it from the package of paint and love videos that often end up on the screen \n",
            "after ['There', 'is', 'a', 'perfect', 'start', 'in', 'Life', 'to', 'distinguish', 'it', 'from', 'the', 'package', 'of', 'paint', 'and', 'love', 'videos', 'that', 'often', 'end', 'up', 'on', 'the', 'screen', '']\n",
            "before ['On', 'the', 'surface', 'a', 'silly', 'comedy', ' ', 'Scotland', ' ', 'PA', 'would', 'be', 'forgettable', 'if', 'it', 'were', 'n t', 'such', 'a', 'clever', 'adaptation', 'of', 'the', 'bard', ' s', 'tragic', 'play', ' '] 28\n",
            "after ['On', 'surface', 'a', 'would', 'forgettable', 'n t', 'clever', 'adaptation', 'the', 'bard', 'tragic']\n",
            "before ['A', 'fine', 'film', ' ', 'but', 'it', 'would', 'be', 'a', 'lot', 'better', 'if', 'it', 'stuck', 'to', 'Betty', 'Fisher', 'and', 'left', 'out', 'the', 'other', 'stories', ' '] 24\n",
            "after ['film', ' ', 'but', 'be', 'a', 'better', 'if', 'stuck', 'other', ' ']\n",
            "before ['You', 'might', 'want', 'to', 'take', 'a', 'reality', 'check', 'before', 'you', 'pay', 'the', 'full', 'ticket', 'price', 'to', 'see', ' ', ' ', 'Simone', ' ', ' ', 'and', 'consider', 'a', 'DVD', 'rental', 'instead', ' '] 29\n",
            "after ['You', 'might', ' ', 'to', 'take', 'a', 'reality', 'price', 'want', 'you', 'pay', 'the', 'full', 'ticket', 'check', 'and', 'see', ' ', 'before', 'Simone', 'rental', ' ', 'to', 'consider', 'a', 'DVD', ' ', 'instead', ' ']\n",
            "before ['Well', 'cast', 'and', 'well', 'directed', ' ', 'a', 'powerful', 'drama', 'with', 'enough', 'sardonic', 'wit', 'to', 'keep', 'it', 'from', 'being', 'maudlin', ' '] 20\n",
            "after ['cast', 'and', 'directed', 'a', 'drama', 'with', 'enough', 'sardonic', 'to', 'it', 'maudlin', ' ']\n",
            "before ['The', 'film', 'sounds', 'like', 'the', 'stuff', 'of', 'lurid', 'melodrama', ' ', 'but', 'what', 'makes', 'it', 'interesting', 'as', 'a', 'character', 'study', 'is', 'the', 'fact', 'that', 'the', 'story', 'is', 'told', 'from', 'Paul', ' s', 'perspective', ' '] 32\n",
            "The film sounds like angry melodrama material, but what interests it about studying the characters is the fact that it is narrated from Paul's perspective. \n",
            "after ['The', 'film', 'sounds', 'like', 'angry', 'melodrama', 'material,', 'but', 'what', 'interests', 'it', 'about', 'studying', 'the', 'characters', 'is', 'the', 'fact', 'that', 'it', 'is', 'narrated', 'from', \"Paul's\", 'perspective.', '']\n",
            "before ['In', 'the', 'disturbingly', 'involving', 'family', 'dysfunctional', 'drama', 'How', 'I', 'Killed', 'My', 'Father', ' ', 'French', 'director', 'Anne', 'Fontaine', 'delivers', 'an', 'inspired', 'portrait', 'of', 'male', ' ', 'ridden', 'angst', 'and', 'the', 'emotional', 'blockage', 'that', 'accompanies', 'this', 'human', 'condition'] 35\n",
            "In an incredibly unusual family drama, How I Killed My Father, French director Anne Fontaine offers a portrait inspired by men's grief and emotional obstruction. That came with this human condition \n",
            "after ['In', 'an', 'incredibly', 'unusual', 'family', 'drama,', 'How', 'I', 'Killed', 'My', 'Father,', 'French', 'director', 'Anne', 'Fontaine', 'offers', 'a', 'portrait', 'inspired', 'by', \"men's\", 'grief', 'and', 'emotional', 'obstruction.', 'That', 'came', 'with', 'this', 'human', 'condition', '']\n",
            "before ['Below', 'may', 'not', 'mark', 'Mr ', 'Twohy', ' s', 'emergence', 'into', 'the', 'mainstream', ' ', 'but', 'his', 'promise', 'remains', 'undiminished', ' '] 18\n",
            "after ['not', ' s', 'emergence', 'the', ' ', ' ']\n",
            "before ['Happily', 'stays', 'close', 'to', 'the', 'ground', 'in', 'a', 'spare', 'and', 'simple', 'manner', 'and', 'does', 'n t', 'pummel', 'us', 'with', 'phony', 'imagery', 'or', 'music', ' '] 23\n",
            "after ['Happily', 'stays', 'to', 'a', 'simple', 'manner', 'does', 'n t', 'phony']\n",
            "before ['For', 'his', 'first', 'attempt', 'at', 'film', 'noir', ' ', 'Spielberg', 'presents', 'a', 'fascinating', 'but', 'flawed', 'look', 'at', 'the', 'near', 'future', ' '] 20\n",
            "For his first attempt at Noir Spielberg shows an interesting but flawed appearance in the near future. \n",
            "after ['For', 'his', 'first', 'attempt', 'at', 'Noir', 'Spielberg', 'shows', 'an', 'interesting', 'but', 'flawed', 'appearance', 'in', 'the', 'near', 'future.', '']\n",
            "before ['it', 'somehow', 'managed', 'to', 'make', 'its', 'way', 'past', 'my', 'crappola', 'radar', 'and', 'find', 'a', 'small', 'place', 'in', 'my', 'heart'] 19\n",
            "after ['it', 'somehow', 'managed', 'make', 'find', 'my', 'way', 'crappola', 'my', 'past', 'radar', 'and', 'to', 'a', 'small', 'place', 'in', 'its', 'heart']\n",
            "before ['It', 'does', 'give', 'a', 'taste', 'of', 'the', 'Burning', 'Man', 'ethos', ' ', 'an', 'appealing', 'blend', 'of', 'counter', ' ', 'cultural', 'idealism', 'and', 'hedonistic', 'creativity', ' '] 23\n",
            "after ['It', ' ', 'give', 'a', 'taste', 'appealing', 'the', 'Man', 'Burning', 'ethos', 'does', 'an', 'of', 'hedonistic', 'and', 'counter', ' ', 'cultural', 'idealism', 'of', 'blend', 'creativity', ' ']\n",
            "before ['The', 'limited', 'sets', 'and', 'small', 'confined', 'and', 'dark', 'spaces', 'also', 'are', 'homages', 'to', 'a', 'classic', 'low', ' ', 'budget', 'film', 'noir', 'movie', ' '] 22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVAuG-izMSNZ"
      },
      "source": [
        "len(train_iterator.dataset.examples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSuazWB1QgWx"
      },
      "source": [
        "len(train_iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQYS3ApcSMag"
      },
      "source": [
        "train_iterator.dataset.examples[8].review"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ub4yW2EoTHKK"
      },
      "source": [
        "bool([1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYn_Ij1CTpRS"
      },
      "source": [
        "vars(train_iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-QEi3iI4L0U",
        "outputId": "ff1e86d7-4bca-42b1-cc34-b42d9e3532dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "help(data.Field)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class Field in module torchtext.data.field:\n",
            "\n",
            "class Field(RawField)\n",
            " |  Defines a datatype together with instructions for converting to Tensor.\n",
            " |  \n",
            " |  Field class models common text processing datatypes that can be represented\n",
            " |  by tensors.  It holds a Vocab object that defines the set of possible values\n",
            " |  for elements of the field and their corresponding numerical representations.\n",
            " |  The Field object also holds other parameters relating to how a datatype\n",
            " |  should be numericalized, such as a tokenization method and the kind of\n",
            " |  Tensor that should be produced.\n",
            " |  \n",
            " |  If a Field is shared between two columns in a dataset (e.g., question and\n",
            " |  answer in a QA dataset), then they will have a shared vocabulary.\n",
            " |  \n",
            " |  Attributes:\n",
            " |      sequential: Whether the datatype represents sequential data. If False,\n",
            " |          no tokenization is applied. Default: True.\n",
            " |      use_vocab: Whether to use a Vocab object. If False, the data in this\n",
            " |          field should already be numerical. Default: True.\n",
            " |      init_token: A token that will be prepended to every example using this\n",
            " |          field, or None for no initial token. Default: None.\n",
            " |      eos_token: A token that will be appended to every example using this\n",
            " |          field, or None for no end-of-sentence token. Default: None.\n",
            " |      fix_length: A fixed length that all examples using this field will be\n",
            " |          padded to, or None for flexible sequence lengths. Default: None.\n",
            " |      dtype: The torch.dtype class that represents a batch of examples\n",
            " |          of this kind of data. Default: torch.long.\n",
            " |      preprocessing: The Pipeline that will be applied to examples\n",
            " |          using this field after tokenizing but before numericalizing. Many\n",
            " |          Datasets replace this attribute with a custom preprocessor.\n",
            " |          Default: None.\n",
            " |      postprocessing: A Pipeline that will be applied to examples using\n",
            " |          this field after numericalizing but before the numbers are turned\n",
            " |          into a Tensor. The pipeline function takes the batch as a list, and\n",
            " |          the field's Vocab.\n",
            " |          Default: None.\n",
            " |      lower: Whether to lowercase the text in this field. Default: False.\n",
            " |      tokenize: The function used to tokenize strings using this field into\n",
            " |          sequential examples. If \"spacy\", the SpaCy English tokenizer is\n",
            " |          used. Default: str.split.\n",
            " |      include_lengths: Whether to return a tuple of a padded minibatch and\n",
            " |          a list containing the lengths of each examples, or just a padded\n",
            " |          minibatch. Default: False.\n",
            " |      batch_first: Whether to produce tensors with the batch dimension first.\n",
            " |          Default: False.\n",
            " |      pad_token: The string token used as padding. Default: \"<pad>\".\n",
            " |      unk_token: The string token used to represent OOV words. Default: \"<unk>\".\n",
            " |      pad_first: Do the padding of the sequence at the beginning. Default: False.\n",
            " |      truncate_first: Do the truncating of the sequence at the beginning. Default: False\n",
            " |      stop_words: Tokens to discard during the preprocessing step. Default: None\n",
            " |      is_target: Whether this field is a target variable.\n",
            " |          Affects iteration over batches. Default: False\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      Field\n",
            " |      RawField\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, sequential=True, use_vocab=True, init_token=None, eos_token=None, fix_length=None, dtype=torch.int64, preprocessing=None, postprocessing=None, lower=False, tokenize=<function Field.<lambda> at 0x7fd1b07f5730>, include_lengths=False, batch_first=False, pad_token='<pad>', unk_token='<unk>', pad_first=False, truncate_first=False, stop_words=None, is_target=False)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  build_vocab(self, *args, **kwargs)\n",
            " |      Construct the Vocab object for this field from one or more datasets.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          Positional arguments: Dataset objects or other iterable data\n",
            " |              sources from which to construct the Vocab object that\n",
            " |              represents the set of possible values for this field. If\n",
            " |              a Dataset object is provided, all columns corresponding\n",
            " |              to this field are used; individual columns can also be\n",
            " |              provided directly.\n",
            " |          Remaining keyword arguments: Passed to the constructor of Vocab.\n",
            " |  \n",
            " |  numericalize(self, arr, device=None)\n",
            " |      Turn a batch of examples that use this field into a Variable.\n",
            " |      \n",
            " |      If the field has include_lengths=True, a tensor of lengths will be\n",
            " |      included in the return value.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          arr (List[List[str]], or tuple of (List[List[str]], List[int])):\n",
            " |              List of tokenized and padded examples, or tuple of List of\n",
            " |              tokenized and padded examples and List of lengths of each\n",
            " |              example if self.include_lengths is True.\n",
            " |          device (str or torch.device): A string or instance of `torch.device`\n",
            " |              specifying which device the Variables are going to be created on.\n",
            " |              If left as default, the tensors will be created on cpu. Default: None.\n",
            " |  \n",
            " |  pad(self, minibatch)\n",
            " |      Pad a batch of examples using this field.\n",
            " |      \n",
            " |      Pads to self.fix_length if provided, otherwise pads to the length of\n",
            " |      the longest example in the batch. Prepends self.init_token and appends\n",
            " |      self.eos_token if those attributes are not None. Returns a tuple of the\n",
            " |      padded list and a list containing lengths of each example if\n",
            " |      `self.include_lengths` is `True` and `self.sequential` is `True`, else just\n",
            " |      returns the padded list. If `self.sequential` is `False`, no padding is applied.\n",
            " |  \n",
            " |  preprocess(self, x)\n",
            " |      Load a single example using this field, tokenizing if necessary.\n",
            " |      \n",
            " |      If the input is a Python 2 `str`, it will be converted to Unicode\n",
            " |      first. If `sequential=True`, it will be tokenized. Then the input\n",
            " |      will be optionally lowercased and passed to the user-provided\n",
            " |      `preprocessing` Pipeline.\n",
            " |  \n",
            " |  process(self, batch, device=None)\n",
            " |      Process a list of examples to create a torch.Tensor.\n",
            " |      \n",
            " |      Pad, numericalize, and postprocess a batch and create a tensor.\n",
            " |      \n",
            " |      Args:\n",
            " |          batch (list(object)): A list of object from a batch of examples.\n",
            " |      Returns:\n",
            " |          torch.autograd.Variable: Processed object given the input\n",
            " |          and custom postprocessing Pipeline.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  dtypes = {torch.float32: <class 'float'>, torch.float64: <class 'float...\n",
            " |  \n",
            " |  vocab_cls = <class 'torchtext.vocab.Vocab'>\n",
            " |      Defines a vocabulary object that will be used to numericalize a field.\n",
            " |      \n",
            " |      Attributes:\n",
            " |          freqs: A collections.Counter object holding the frequencies of tokens\n",
            " |              in the data used to build the Vocab.\n",
            " |          stoi: A collections.defaultdict instance mapping token strings to\n",
            " |              numerical identifiers.\n",
            " |          itos: A list of token strings indexed by their numerical identifiers.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from RawField:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTh6ev9b66gV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}